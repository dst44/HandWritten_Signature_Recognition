{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Search for the Best Model ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropoout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.333)\n",
    "sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the dataset\n",
    "\n",
    "pickle_in = open(\"X.pickle\" , \"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"Y.pickle\" , \"rb\")\n",
    "y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that our dataset is grayscale therefore\n",
    "#pixel intensity ranges from 0 to 255\n",
    "#otherwise we might have to normalize data accordingly (keras.utils.normalize)\n",
    "\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [1,2,3,4]\n",
    "layer_sizes = [32,64,128,256]\n",
    "conv_layers = [3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we can change :\n",
    "# 1. no of layers\n",
    "# 2. nodes per layer\n",
    "# 3. no. of dense layers\n",
    "# 4. optimizer\n",
    "# 5. loss fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            \n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer,layer_size,dense_layer, int(time.time()))\n",
    "            tensorboard = TensorBoard(Log_dir = 'logs/{}'.format(NAME)) \n",
    "            \n",
    "            \n",
    "            #this saves our model in a new library named NAMED\n",
    "            #we can then visualize all the graphs by doing the foll\n",
    "            # 1. go to cmd, make sure the directory that contains this tensorboard the one in which cmd is running\n",
    "            # 2. type tensorboard --logdir = logs/\n",
    "            # 3. you get an https link , open it in browser and you get all graphs with validation accuracy v/s epochs etc.\n",
    "            # 4. look at validation loss and optimise the model...like change parameters, change dense layers etc\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add( Conv2D(64) , (3,3) ,input_shape = X.shape[1:] ) #dynamically assigning shape\n",
    "            model.add(Activation(\"relu\"))\n",
    "            model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "            \n",
    "            for l in range(conv_layer-1)\n",
    "                model.add(Conv2D(layer_size) , (3,3) ) #don't need to assign shape anymore\n",
    "                model.add(Activation(\"relu\"))\n",
    "                model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "            \n",
    "            for l in range(dense_layer)\n",
    "                model.add(Dense(dense_layer))\n",
    "                model.add(Activation(\"relu\"))\n",
    "                model.add(Dropout(0.2)) # 20% dropout to prevent overfitting\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation(\"sigmoid\"))\n",
    "            \n",
    "            model.compile(loss = \"binary_crossentropy\",\n",
    "            optimizer = \"adam\",\n",
    "            metrics = [\"accuracy\"])\n",
    "\n",
    "            model.fit(X,y,batch_size = 20 , epochs = 5 , validation_split = 0.2 , callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many dense layers sometimes memorizes the dataset if it is small \n",
    "# this can result in overfitting and too high an accuracy sometimes\n",
    "#this took around 45min to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we found best fit at 32,64,64,128 filters ->4 CNN , 2 DNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
